@ARTICLE{7501805, 
author={P. Punpongsanon and E. Guy and D. Iwai and K. Sato and T. Boubekeur}, 
journal={IEEE Transactions on Visualization and Computer Graphics}, 
title={Extended LazyNav: Virtual 3D Ground Navigation for Large Displays and Head-Mounted Displays}, 
year={2017}, 
volume={23}, 
number={8}, 
pages={1952-1963}, 
abstract={This paper presents the extended work on LazyNav, a head-free, eyes-free and hands-free mid-air ground navigation control model presented at the IEEE 3D User Interfaces (3DUI) 2015, in particular with a new application to the head-mounted display (HMD). Our mid-air interaction metaphor makes use of only a single pair of the remaining tracked body elements to tailor the navigation. Therefore, the user can navigate in the scene while still being able to perform other interactions with her hands and head, e.g., carrying a bag, grasping a cup of coffee, or observing the content by moving her eyes and locally rotating her head. We design several body motions for navigation by considering the use of non-critical body parts and develop assumptions about ground navigation techniques. Through the user studies, we investigate the motions that are easy to discover, easy to control, socially acceptable, accurate and not tiring. Finally, we evaluate the desired ground navigation features with a prototype application in both a large display (LD) and a HMD navigation scenarios. We highlight several recommendations for designing a particular mid-air ground navigation technique for a LD and a HMD.}, 
summary={In this paper the authors propose a series of mid-air gestures to be used for navigation in 3D environments which can be applied to either a HMD environment or large-format display (LD) setting.  The goals of the gestures are to enable locomotion by satisfying the criteria of: social acceptance, effort, and the ability to perform secondary actions while performing the gesture.  The authors devised a series of gestures separated into two categories, movement and view.  Through user studies, they tested many combinations of movement and view gestures (called a motion pair).  They found that motions that are most coorelated to the real movement achieved greater success.  They also found that pairs that are in uncoorelated planes also performed better (e.g. rotate hips for view and step for walk were undesirable due to the relation in movement).  The authors conculded that the LD motions were generally more favorable than HMD motions.  This could be due to technical limitations of current HMDs and possibly lack of familiarity with the technology.  The authors also concluded that there are motion pairs which are able to satisfy their criteria.  There are, of course, trade-offs to be made between the various pairs.  Overall, modifying view by leaning the bust and walking by bending the knees was the most successful combination for HMDs.  },
keywords={helmet mounted displays;human computer interaction;navigation;user interfaces;virtual reality;3DUI;HMD;IEEE 3D user interfaces;LazyNav;body motions;eyes-free mid-air ground navigation control model;hands-free mid-air ground navigation control model;head-free mid-air ground navigation control model;head-mounted displays;large display;mid-air interaction metaphor;motion control;noncritical body parts;virtual 3D ground navigation;Hip;Legged locomotion;Navigation;Sensors;Space vehicles;Three-dimensional displays;Tracking;3D user interface;navigation;spatial interaction;virtual reality}, 
doi={10.1109/TVCG.2016.2586071}, 
ISSN={1077-2626}, 
month={Aug},}